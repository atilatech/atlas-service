{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atilatech/atlas-service/blob/master/notebooks/whisper_model_deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfGnwnYcEgw4"
      },
      "source": [
        "# Deploy Open AI Model\n",
        "\n",
        "This notebook shows how to save and upload a model to s3.\n",
        "\n",
        "The plan is to use this to deploy a model on Sagemaker.\n",
        "\n",
        "1. Create model on local machine\n",
        "2. Save model using joblib\n",
        "3. Verify that saved model works\n",
        "4. Upload to S3\n",
        "\n",
        "> Inspired by [Deploy A Locally Trained ML Model In Cloud Using AWS SageMaker](https://medium.com/geekculture/84af8989d065)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MdqopKPErwk"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJcVS-NsEaqY"
      },
      "outputs": [],
      "source": [
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EXO7ZoZFTFV"
      },
      "outputs": [],
      "source": [
        "import pytube\n",
        "\n",
        "url = \"https://www.youtube.com/watch?v=bGk8qcHc1A0\" # Joe Rogan & Lex Fridman: Lionel Messi Is The GOAT Over Cristiano Ronaldo\n",
        "yt = pytube.YouTube(url)\n",
        "# yt.streams.filter(only_audio=True).first()\\\n",
        "# .download(output_path='mp3', filename=f\"{yt.video_id}.mp3\")\n",
        "itag = None\n",
        "files = yt.streams.filter(only_audio=True)\n",
        "for file in files:\n",
        "    # from audio files we grab the first audio for mp4 (eg mp3)\n",
        "    if file.mime_type == 'audio/mp4':\n",
        "        itag = file.itag\n",
        "        break\n",
        "    if itag is None:\n",
        "        # just incase no MP3 audio is found (shouldn't happen)\n",
        "        print(\"NO MP3 AUDIO FOUND\")\n",
        "        continue\n",
        "\n",
        "# get the correct mp3 'stream'\n",
        "stream = yt.streams.get_by_itag(itag)\n",
        "# downloading the audio\n",
        "stream.download(\n",
        "    output_path='mp3',\n",
        "    filename=f\"{yt.video_id}.mp3\"\n",
        ")\n",
        "\n",
        "# Add the video info to the list of downloaded videos\n",
        "video_info = {\n",
        "    'id': yt.video_id,\n",
        "    'thumbnail': yt.thumbnail_url,\n",
        "    'title': yt.title,\n",
        "    'views': yt.views,\n",
        "    'length': yt.length,\n",
        "}\n",
        "video_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkLOZ-QnIXSW"
      },
      "source": [
        "# Transcribe Audio\n",
        "1. Download model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AT2rg45Nw58",
        "outputId": "1fbd6800-61fe-4ed3-d683-d78587c7dd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.8 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 57.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 63.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 59.5 MB/s \n",
            "\u001b[?25h  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "!apt install ffmpeg # https://stackoverflow.com/questions/51856340/how-to-install-package-ffmpeg-in-google-colab\n",
        "\n",
        "# optional install pytorch so you can use a gpu for faster transcription\n",
        "# command below is for Linux. See instructions for mac and windows: https://pytorch.org/get-started/locally/\n",
        "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTX0Zvj5NyR0"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import torch  # install steps: pytorch.org\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f'whisper will use: {device}')\n",
        "\n",
        "large_gpu_model = whisper.load_model(\"large\").to(\"cuda\")\n",
        "# large_cpu_model = whisper.load_model(\"large\").to(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ol3SuPU_FZO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T807rOf9Y1PX"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "audio_paths = [str(x) for x in Path('./mp3').glob('*.mp3')]\n",
        "audio_path = audio_paths[0]\n",
        "\n",
        "# verbose: bool\n",
        "# Whether to display the text being decoded to the console. \n",
        "# If True, displays all the details (live transcription)\n",
        "# If False, displays minimal details. (progress bar)\n",
        "# If None, does not display anything\n",
        "# Only show live transcript if video length is less than 300 seconsd (5 minutes)\n",
        "# To avoid too much text in console\n",
        "verbose = True if yt.length <= 300 else False\n",
        "audio_transcript = large_gpu_model.transcribe(audio_path,verbose=verbose)\n",
        "text = audio_transcript['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model\n",
        "\n",
        "Save the model using joblib then"
      ],
      "metadata": {
        "id": "vRi5Hk7_J3rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_transcript\n",
        "\n",
        "model_file_name = 'whisper-large_gpu_model'\n",
        "import joblib\n",
        "joblib.dump(large_gpu_model, model_file_name)"
      ],
      "metadata": {
        "id": "x-cbNLNICaCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "large_gpu_model_dumped = joblib.load(model_file_name)"
      ],
      "metadata": {
        "id": "17_yzeOnIMI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_transcript_2 = large_gpu_model_dumped.transcribe(audio_path,verbose=True)"
      ],
      "metadata": {
        "id": "H3yjqn77IkN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the file size in bytes\n",
        "file_size_bytes = os.path.getsize(model_file_name)\n",
        "\n",
        "# Convert the file size to GB\n",
        "file_size_gb = file_size_bytes / (1024 ** 3)\n",
        "\n",
        "# Convert the file size to MB\n",
        "file_size_mb = file_size_bytes / (1024 ** 2)\n",
        "\n",
        "print(f\"File size: {file_size_gb:.2f} GB ({file_size_mb:.2f} MB)\")\n"
      ],
      "metadata": {
        "id": "APAxliO4Kw8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model to Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path_in_google_drive = f'/content/drive/MyDrive/Atlas-models/{model_file_name}'\n",
        "joblib.dump(large_gpu_model, model_path_in_google_drive)"
      ],
      "metadata": {
        "id": "pMwqRGJ5MUmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Model to S3\n",
        "\n",
        "This is meant to be a separate section run if you want to start a new\n",
        "session with an existing model that has been saved with joblib.\n",
        "\n",
        "The file will be about 5.75 GB big so we'll want to upload to S3 from Colab.\n",
        "\n",
        "[S3 Multipart Upload Limits](https://docs.aws.amazon.com/AmazonS3/latest/userguide/qfacts.html)\n",
        "\n",
        "## Compress to Google Drive\n",
        "\n",
        "Compress data because ran into Maximum part size - 5GB\n",
        "\n",
        "- [Zlib compression](https://joblib.readthedocs.io/en/latest/persistence.html#compressed-joblib-pickles:~:text=By%20default%2C%20joblib.dump()%20uses%20the%20zlib%20compression%20method%20as%20it%20gives%20the%20best%20tradeoff%20between%20speed%20and%20disk%20space.)\n",
        "\n",
        "- [Comparison of different compressors](https://joblib.readthedocs.io/en/latest/auto_examples/compressors_comparison.html#sphx-glr-auto-examples-compressors-comparison-py)\n",
        "\n",
        "Loading Raw file: 59 seconds\n",
        "File Size: 5.75 GB\n",
        "\n",
        "|                    | Raw  | Compressed |\n",
        "|--------------------|------|------------|\n",
        "| File Size (GB)     | 5.75 |       3.45 |\n",
        "| Save Time (s)      | ?    |        489 |\n",
        "| Load Time (s)      |   59 |         66 |\n",
        "| Inference Time (s) |    ? |         54 |\n",
        "\n",
        "? = That value hasn't been recorded.\n",
        "\n",
        "## Use Model in Sagemaker\n",
        "\n",
        "1. Create a Sagemaker Instance and"
      ],
      "metadata": {
        "id": "kLvKktPMLqNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "model_file_name = 'whisper-large_gpu_model'\n",
        "model_path_in_google_drive = f'/content/drive/MyDrive/Atlas-models/{model_file_name}'\n",
        "large_gpu_model_dumped = joblib.load(model_path_in_google_drive)\n",
        "\n",
        "large_gpu_model_dumped"
      ],
      "metadata": {
        "id": "xc5Glptls4QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compress the file because it is 5.75 GB uncompressed\n",
        "# AWS has a Maximum Part Size of 5GB\n",
        "# https://joblib.readthedocs.io/en/latest/persistence.html#compressed-joblib-pickles:~:text=By%20default%2C%20joblib.dump()%20uses%20the%20zlib%20compression%20method%20as%20it%20gives%20the%20best%20tradeoff%20between%20speed%20and%20disk%20space.\n",
        "# https://joblib.readthedocs.io/en/latest/auto_examples/compressors_comparison.html#sphx-glr-auto-examples-compressors-comparison-py\n",
        "# Use zlib because it has the best tradeoff between size and speed\n",
        "model_path_in_google_drive = f'/content/drive/MyDrive/Atlas-models/{model_file_name}'\n",
        "\n",
        "model_path_in_google_drive_compressed = model_path_in_google_drive + '.gz'\n",
        "joblib.dump(large_gpu_model_dumped,\n",
        "            model_path_in_google_drive_compressed,\n",
        "            compress=True)\n"
      ],
      "metadata": {
        "id": "LL5La_-s-xSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the file size in bytes\n",
        "file_size_bytes = os.path.getsize(model_path_in_google_drive_compressed)\n",
        "\n",
        "# Convert the file size to GB\n",
        "file_size_gb = file_size_bytes / (1024 ** 3)\n",
        "\n",
        "# Convert the file size to MB\n",
        "file_size_mb = file_size_bytes / (1024 ** 2)\n",
        "\n",
        "print(f\"File size: {file_size_gb:.2f} GB ({file_size_mb:.2f} MB)\")"
      ],
      "metadata": {
        "id": "R84_bOvAAtm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "large_gpu_model_dumped_compressed = joblib.load(model_path_in_google_drive_compressed)"
      ],
      "metadata": {
        "id": "nup-YWYQBMUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "large_gpu_model_dumped_compressed"
      ],
      "metadata": {
        "id": "2ejH9b4vCOOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "audio_paths = [str(x) for x in Path('./mp3').glob('*.mp3')]\n",
        "audio_path = audio_paths[0]\n",
        "\n",
        "\n",
        "decode_options = {\n",
        "     # Set language to None to support multilingual, \n",
        "     # but it will take longer to process while it detects the language.\n",
        "     # Realized this by running in verbose mode and seeing how much time\n",
        "     # was spent on the decoding language step\n",
        "    \"language\":\"en\"\n",
        "} \n",
        "audio_transcript = large_gpu_model_dumped_compressed.transcribe(audio_path,\n",
        "                                                     verbose=True,\n",
        "                                                     **decode_options)\n",
        "audio_transcript"
      ],
      "metadata": {
        "id": "6ogcpc5wuL2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "id": "yYn53dKVq9ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add AWS Credentials\n",
        "\n",
        "1.  [Set AWS credentials](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) in your [environment variable](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#environment-variables) \n",
        "1. [Set default region](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html?highlight=aws_default_region#using-environment-variables).\n",
        "\n",
        "See: [Passing secret variables to Google colaboratory network](https://stackoverflow.com/questions/51058533/passing-secret-variables-to-google-colaboratory-notebook/74892619#74892619),  [How do I set environment variables in Google Colab?](https://stackoverflow.com/questions/66631333/how-do-i-set-environment-variables-in-google-colab) and [How to get the region of the current user from boto?](https://stackoverflow.com/questions/37514810/how-to-get-the-region-of-the-current-user-from-boto)\n",
        "\n"
      ],
      "metadata": {
        "id": "yTUY7RqTctD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "AWS_ACCESS_KEY_ID = getpass('Enter AWS_ACCESS_KEY_ID')\n",
        "AWS_SECRET_ACCESS_KEY = getpass('Enter AWS_SECRET_ACCESS_KEY')\n",
        "\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = AWS_ACCESS_KEY_ID\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = AWS_SECRET_ACCESS_KEY\n",
        "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4uHGuY1kvBE",
        "outputId": "1d421762-ac95-4ce2-c6cc-fe337fa07adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter AWS_ACCESS_KEY_ID··········\n",
            "Enter AWS_SECRET_ACCESS_KEY··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify that your credentials work.\n",
        "\n",
        "You can also run: `sts.get_caller_identity()` to see more info about the user\n",
        "running the command."
      ],
      "metadata": {
        "id": "9AZISQw7gR0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "sts = boto3.client(\"sts\")\n",
        "# sts.get_caller_identity() # commented out to avoid revealing sensitive info\n",
        "\n",
        "print(\"Authentication HTTP Status:\",\n",
        "      sts.get_caller_identity()['ResponseMetadata']['HTTPStatusCode'])\n",
        "print(\"active region:\",\n",
        "      sts.meta.region_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhqok7Odfheo",
        "outputId": "f096f9ec-6516-44d0-f78f-65aafec09dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authentication HTTP Status: 200\n",
            "active region: us-east-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that the credentials work. \n",
        "# This won't verify that you have upload9write access to the bucket\n",
        "\n",
        "import logging\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "import os\n",
        "\n",
        "# See progress of the upload\n",
        "\n",
        "  # Upload the file\n",
        "s3_client = boto3.client('s3')\n"
      ],
      "metadata": {
        "id": "jfTUg1lmmdd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify your connection\n",
        "s3_client.list_buckets()['Buckets'][:5]"
      ],
      "metadata": {
        "id": "_K4K7LvqxIRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import threading\n",
        "# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html#the-callback-parameter\n",
        "class ProgressPercentage(object):\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self._filename = filename\n",
        "        self._size = float(os.path.getsize(filename))\n",
        "        self._seen_so_far = 0\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "    def __call__(self, bytes_amount):\n",
        "        # To simplify, assume this is hooked up to a single filename\n",
        "        with self._lock:\n",
        "            self._seen_so_far += bytes_amount\n",
        "            percentage = (self._seen_so_far / self._size) * 100\n",
        "            # You probably don't need both print and sys.stdout.write\n",
        "            print(\"\\r%s  %s / %s  (%.2f%%)\" % (\n",
        "                    self._filename, self._seen_so_far, self._size,\n",
        "                    percentage))\n",
        "            sys.stdout.write(\n",
        "                \"\\r%s  %s / %s  (%.2f%%)\" % (\n",
        "                    self._filename, self._seen_so_far, self._size,\n",
        "                    percentage))\n",
        "            sys.stdout.flush()"
      ],
      "metadata": {
        "id": "HRezjIWandnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name='atila-ai-models-2'# your bucket name here\n",
        "model_path_in_google_drive = f'/content/drive/MyDrive/Atlas-models/{model_file_name}'\n",
        "\n",
        "model_path_in_google_drive_compressed = model_path_in_google_drive + '.gz'\n",
        "try:\n",
        "    response = s3_client.upload_file(model_path_in_google_drive_compressed, \n",
        "                                     bucket_name,\n",
        "                                     os.path.basename(model_path_in_google_drive_compressed),\n",
        "                                     Callback=ProgressPercentage(model_path_in_google_drive_compressed))\n",
        "except ClientError as e:\n",
        "    logging.error(e)"
      ],
      "metadata": {
        "id": "ZG0uzXXyjoLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.basename(model_path_in_google_drive_compressed)"
      ],
      "metadata": {
        "id": "T1L4RZfjN2E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running in Sagemaker\n",
        "\n",
        "1. [Available EC2 Instances in Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html)\n",
        "\n",
        "1. [ml.g4dn.xlarge](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html#:~:text=ml.p3dn.24xlarge-,ml.g4dn.xlarge,-%3E%3E%20Fast%20launch) because it is the only GPU with fast launch\n",
        "  1. Uses [Elastic Inference](https://aws.amazon.com/machine-learning/elastic-inference/)\n",
        "\n",
        "1. [EC2 Instances](https://aws.amazon.com/ec2/instance-types/)\n",
        "\n",
        "Note: If you try to add Elastic inference you might need to request a service\n",
        "limit, which may take a few days.\n",
        "\n",
        "https://stackoverflow.com/questions/71738894/unable-to-create-aws-segamaker-error-the-account-level-service-limit-number-o\n",
        "\n",
        "https://support.console.aws.amazon.com/support/home?region=us-east-1&skipRegion=true#/case/create?issueType=service-limit-increase\n",
        "\n",
        "https://discuss.huggingface.co/t/deploying-open-ais-whisper-on-sagemaker/24761/16\n",
        "\n",
        "https://stackoverflow.com/questions/56255154/how-to-use-a-pretrained-model-from-s3-to-predict-some-data"
      ],
      "metadata": {
        "id": "KwmJ6IOHRBvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sagemaker\n",
        "\n"
      ],
      "metadata": {
        "id": "_lwHh7i6YxGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "from sagemaker.tensorflow import TensorFlowModel\n",
        "from sagemaker.utils import S3DataConfig\n",
        "\n",
        "import shutil\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
        "\n",
        "# role = sagemaker.get_execution_role()\n",
        "sm_session = sagemaker.Session()\n",
        "bucket_name = sm_session.default_bucket()"
      ],
      "metadata": {
        "id": "M5xeV7nnaykI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNh86Uo8MgnGqzDLWeCzMRh",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}