{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atilatech/atlas-service/blob/master/notebooks/whisper_model_deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfGnwnYcEgw4"
      },
      "source": [
        "# Deploy Open AI Model\n",
        "\n",
        "This notebook shows how to save and upload a model to s3.\n",
        "\n",
        "1. Create model on local machine\n",
        "2. Save model using joblib\n",
        "3. Verify that saved model works\n",
        "4. Upload to S3\n",
        "\n",
        "> Inspired by [Deploy A Locally Trained ML Model In Cloud Using AWS SageMaker](https://medium.com/geekculture/84af8989d065)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MdqopKPErwk"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJcVS-NsEaqY",
        "outputId": "df2ec6ec-394f-43a3-bae3-69ddc7c40330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.8/dist-packages (12.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0EXO7ZoZFTFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f26c58-934c-495d-f332-1b116fe3444a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'bGk8qcHc1A0',\n",
              " 'thumbnail': 'https://i.ytimg.com/vi/bGk8qcHc1A0/sddefault.jpg',\n",
              " 'title': 'Joe Rogan & Lex Fridman: Lionel Messi Is The GOAT Over Cristiano Ronaldo',\n",
              " 'views': 185138,\n",
              " 'length': 218}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pytube\n",
        "\n",
        "url = \"https://www.youtube.com/watch?v=bGk8qcHc1A0\" # Joe Rogan & Lex Fridman: Lionel Messi Is The GOAT Over Cristiano Ronaldo\n",
        "yt = pytube.YouTube(url)\n",
        "# yt.streams.filter(only_audio=True).first()\\\n",
        "# .download(output_path='mp3', filename=f\"{yt.video_id}.mp3\")\n",
        "itag = None\n",
        "files = yt.streams.filter(only_audio=True)\n",
        "for file in files:\n",
        "    # from audio files we grab the first audio for mp4 (eg mp3)\n",
        "    if file.mime_type == 'audio/mp4':\n",
        "        itag = file.itag\n",
        "        break\n",
        "    if itag is None:\n",
        "        # just incase no MP3 audio is found (shouldn't happen)\n",
        "        print(\"NO MP3 AUDIO FOUND\")\n",
        "        continue\n",
        "\n",
        "# get the correct mp3 'stream'\n",
        "stream = yt.streams.get_by_itag(itag)\n",
        "# downloading the audio\n",
        "stream.download(\n",
        "    output_path='mp3',\n",
        "    filename=f\"{yt.video_id}.mp3\"\n",
        ")\n",
        "\n",
        "# Add the video info to the list of downloaded videos\n",
        "video_info = {\n",
        "    'id': yt.video_id,\n",
        "    'thumbnail': yt.thumbnail_url,\n",
        "    'title': yt.title,\n",
        "    'views': yt.views,\n",
        "    'length': yt.length,\n",
        "}\n",
        "video_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkLOZ-QnIXSW"
      },
      "source": [
        "# Transcribe Audio\n",
        "1. Download model\n",
        "\n",
        "\n",
        "Tip: [Use](https://stackoverflow.com/questions/51058533/passing-secret-variables-to-google-colaboratory-notebook/74892619#74892619) `getpass` for passing secret environment variables. Not needed here bu writing\n",
        "it so we don't forget."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AT2rg45Nw58",
        "outputId": "2df0d182-9456-41c8-d8ea-287387728141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.8 MB 34.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 51.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 47.1 MB/s \n",
            "\u001b[?25h  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "!apt install ffmpeg # https://stackoverflow.com/questions/51856340/how-to-install-package-ffmpeg-in-google-colab\n",
        "\n",
        "# optional install pytorch so you can use a gpu for faster transcription\n",
        "# command below is for Linux. See instructions for mac and windows: https://pytorch.org/get-started/locally/\n",
        "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTX0Zvj5NyR0",
        "outputId": "a217efde-39b9-4c8e-9bab-462cc1af4d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whisper will use: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 2.87G/2.87G [00:25<00:00, 122MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "import torch  # install steps: pytorch.org\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f'whisper will use: {device}')\n",
        "\n",
        "large_gpu_model = whisper.load_model(\"large\").to(\"cuda\")\n",
        "# large_cpu_model = whisper.load_model(\"large\").to(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ol3SuPU_FZO_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T807rOf9Y1PX"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "audio_paths = [str(x) for x in Path('./mp3').glob('*.mp3')]\n",
        "audio_path = audio_paths[0]\n",
        "\n",
        "# verbose: bool\n",
        "# Whether to display the text being decoded to the console. \n",
        "# If True, displays all the details (live transcription)\n",
        "# If False, displays minimal details. (progress bar)\n",
        "# If None, does not display anything\n",
        "# Only show live transcript if video length is less than 300 seconsd (5 minutes)\n",
        "# To avoid too much text in console\n",
        "verbose = True if yt.length <= 300 else False\n",
        "audio_transcript = large_gpu_model.transcribe(audio_path,verbose=verbose)\n",
        "text = audio_transcript['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model\n",
        "\n",
        "Save the model using joblib then"
      ],
      "metadata": {
        "id": "vRi5Hk7_J3rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_transcript\n",
        "\n",
        "model_file_name = 'whisper-large_gpu_model'\n",
        "import joblib\n",
        "joblib.dump(large_gpu_model, model_file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-cbNLNICaCs",
        "outputId": "20fd9571-4bc6-4141-b121-b484c73c756a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['whisper-large_gpu_model']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "large_gpu_model_dumped = joblib.load(model_file_name)"
      ],
      "metadata": {
        "id": "17_yzeOnIMI0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_transcript_2 = large_gpu_model_dumped.transcribe(audio_path,verbose=True)"
      ],
      "metadata": {
        "id": "H3yjqn77IkN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the file size in bytes\n",
        "file_size_bytes = os.path.getsize(model_file_name)\n",
        "\n",
        "# Convert the file size to GB\n",
        "file_size_gb = file_size_bytes / (1024 ** 3)\n",
        "\n",
        "# Convert the file size to MB\n",
        "file_size_mb = file_size_bytes / (1024 ** 2)\n",
        "\n",
        "print(f\"File size: {file_size_gb:.2f} GB ({file_size_mb:.2f} MB)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APAxliO4Kw8-",
        "outputId": "51a9d839-f011-4eec-82ca-eb6b7e8c722f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 5.75 GB (5888.47 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model to Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path_in_google_drive = f'/content/drive/MyDrive/Atlas-models/{model_file_name}'\n",
        "joblib.dump(large_gpu_model, model_path_in_google_drive)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMwqRGJ5MUmh",
        "outputId": "73545d09-0b25-4859-c9cc-f88ab7915e6d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Atlas-models/whisper-large_gpu_model']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Model to S3\n",
        "\n",
        "This is meant to be a separate section run if you want to start a new\n",
        "session with an existing model that has been saved with joblib.\n",
        "\n",
        "The file will be about 5.75 GB big so we'll want to upload to S3 from Colab.\n",
        "\n",
        "[S3 Multipart Upload Limits](https://docs.aws.amazon.com/AmazonS3/latest/userguide/qfacts.html)\n",
        "\n",
        "## Compress to Google Drive\n",
        "\n",
        "Compress data because ran into Maximum part size - 5GB\n",
        "\n",
        "- [Zlib compression](https://joblib.readthedocs.io/en/latest/persistence.html#compressed-joblib-pickles:~:text=By%20default%2C%20joblib.dump()%20uses%20the%20zlib%20compression%20method%20as%20it%20gives%20the%20best%20tradeoff%20between%20speed%20and%20disk%20space.)\n",
        "\n",
        "- [Comparison of different compressors](https://joblib.readthedocs.io/en/latest/auto_examples/compressors_comparison.html#sphx-glr-auto-examples-compressors-comparison-py)\n",
        "\n",
        "Loading Raw file: 59 seconds\n",
        "File Size: 5.75 GB\n",
        "\n",
        "|                    | Raw  | Compressed |\n",
        "|--------------------|------|------------|\n",
        "| File Size (GB)     | 5.75 |       3.45 |\n",
        "| Save Time (s)      | ?    |        489 |\n",
        "| Load Time (s)      |   59 |         66 |\n",
        "| Inference Time (s) |    ? |         54 |\n",
        "\n",
        "? = That value hasn't been recorded.\n",
        "\n",
        "2. Upload to S3\n",
        "  1. [Get S3 credentials](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)\n",
        "\n",
        "\n",
        "\n",
        "## Use Model in Sagemaker\n",
        "\n",
        "1. Create a Sagemaker Instance and"
      ],
      "metadata": {
        "id": "kLvKktPMLqNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "model_file_name = 'whisper-large_gpu_model'\n",
        "model_path_in_google_drive = f'/content/drive/MyDrive/Atlas-models/{model_file_name}'\n",
        "large_gpu_model_dumped = joblib.load(model_path_in_google_drive)\n",
        "\n",
        "large_gpu_model_dumped"
      ],
      "metadata": {
        "id": "xc5Glptls4QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compress the file because it is 5.75 GB uncompressed\n",
        "# AWS has a Maximum Part Size of 5GB\n",
        "# https://joblib.readthedocs.io/en/latest/persistence.html#compressed-joblib-pickles:~:text=By%20default%2C%20joblib.dump()%20uses%20the%20zlib%20compression%20method%20as%20it%20gives%20the%20best%20tradeoff%20between%20speed%20and%20disk%20space.\n",
        "# https://joblib.readthedocs.io/en/latest/auto_examples/compressors_comparison.html#sphx-glr-auto-examples-compressors-comparison-py\n",
        "# Use zlib because it has the best tradeoff between size and speed\n",
        "model_path_in_google_drive = f'/content/drive/MyDrive/Atlas-models/{model_file_name}'\n",
        "\n",
        "model_path_in_google_drive_compressed = model_path_in_google_drive + '.gz'\n",
        "joblib.dump(large_gpu_model_dumped,\n",
        "            model_path_in_google_drive_compressed,\n",
        "            compress=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL5La_-s-xSI",
        "outputId": "dd43d40a-8ccf-4b0f-84ae-b4209873f732"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Atlas-models/whisper-large_gpu_model.gz']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the file size in bytes\n",
        "file_size_bytes = os.path.getsize(model_path_in_google_drive_compressed)\n",
        "\n",
        "# Convert the file size to GB\n",
        "file_size_gb = file_size_bytes / (1024 ** 3)\n",
        "\n",
        "# Convert the file size to MB\n",
        "file_size_mb = file_size_bytes / (1024 ** 2)\n",
        "\n",
        "print(f\"File size: {file_size_gb:.2f} GB ({file_size_mb:.2f} MB)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R84_bOvAAtm3",
        "outputId": "b382e1c1-177b-40ff-f242-cab869dfe043"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 3.45 GB (3536.86 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "large_gpu_model_dumped_compressed = joblib.load(model_path_in_google_drive_compressed)"
      ],
      "metadata": {
        "id": "nup-YWYQBMUz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "large_gpu_model_dumped_compressed"
      ],
      "metadata": {
        "id": "2ejH9b4vCOOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "audio_paths = [str(x) for x in Path('./mp3').glob('*.mp3')]\n",
        "audio_path = audio_paths[0]\n",
        "\n",
        "\n",
        "decode_options = {\n",
        "     # Set language to None to support multilingual, \n",
        "     # but it will take longer to process while it detects the language.\n",
        "     # Realized this by running in verbose mode and seeing how much time\n",
        "     # was spent on the decoding language step\n",
        "    \"language\":\"en\"\n",
        "} \n",
        "audio_transcript = large_gpu_model_dumped_compressed.transcribe(audio_path,\n",
        "                                                     verbose=True,\n",
        "                                                     **decode_options)\n",
        "audio_transcript"
      ],
      "metadata": {
        "id": "6ogcpc5wuL2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "id": "yYn53dKVq9ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "AWS_ACCESS_KEY = getpass('Enter AWS_ACCESS_KEY')\n",
        "AWS_SECRET_KEY = getpass('Enter AWS_SECRET_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4uHGuY1kvBE",
        "outputId": "8d3d18f0-449c-47e2-d2cb-e9395adf880e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter AWS_ACCESS_KEY··········\n",
            "Enter AWS_SECRET_KEY··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that the credentials work. \n",
        "# This won't verify that you have upload9write access to the bucket\n",
        "\n",
        "import logging\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "import os\n",
        "\n",
        "# See progress of the upload\n",
        "\n",
        "  # Upload the file\n",
        "s3_client = boto3.client(\n",
        "  's3',\n",
        "  aws_access_key_id=AWS_ACCESS_KEY,\n",
        "  aws_secret_access_key=AWS_SECRET_KEY,\n",
        ")\n"
      ],
      "metadata": {
        "id": "jfTUg1lmmdd6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify your connection\n",
        "s3_client.list_buckets()['Buckets'][:5]"
      ],
      "metadata": {
        "id": "_K4K7LvqxIRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import threading\n",
        "# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html#the-callback-parameter\n",
        "class ProgressPercentage(object):\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self._filename = filename\n",
        "        self._size = float(os.path.getsize(filename))\n",
        "        self._seen_so_far = 0\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "    def __call__(self, bytes_amount):\n",
        "        # To simplify, assume this is hooked up to a single filename\n",
        "        with self._lock:\n",
        "            self._seen_so_far += bytes_amount\n",
        "            percentage = (self._seen_so_far / self._size) * 100\n",
        "            # You probably don't need both print and sys.stdout.write\n",
        "            print(\"\\r%s  %s / %s  (%.2f%%)\" % (\n",
        "                    self._filename, self._seen_so_far, self._size,\n",
        "                    percentage))\n",
        "            sys.stdout.write(\n",
        "                \"\\r%s  %s / %s  (%.2f%%)\" % (\n",
        "                    self._filename, self._seen_so_far, self._size,\n",
        "                    percentage))\n",
        "            sys.stdout.flush()"
      ],
      "metadata": {
        "id": "HRezjIWandnG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name='atila-ai-models-2'# your bucket name here\n",
        "model_path_in_google_drive = f'/content/drive/MyDrive/Atlas-models/{model_file_name}'\n",
        "\n",
        "model_path_in_google_drive_compressed = model_path_in_google_drive + '.gz'\n",
        "try:\n",
        "    response = s3_client.upload_file(model_path_in_google_drive_compressed, \n",
        "                                     bucket_name,\n",
        "                                     os.path.basename(model_path_in_google_drive_compressed),\n",
        "                                     Callback=ProgressPercentage(model_path_in_google_drive_compressed))\n",
        "except ClientError as e:\n",
        "    logging.error(e)"
      ],
      "metadata": {
        "id": "ZG0uzXXyjoLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.basename(model_path_in_google_drive_compressed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T1L4RZfjN2E8",
        "outputId": "ec3ba450-31a9-4a84-9f89-6a5ce6d42d17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'whisper-large_gpu_model.gz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running in Sagemaker\n",
        "\n",
        "1. [Available EC2 Instances in Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html)\n",
        "\n",
        "1. [ml.g4dn.xlarge](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html#:~:text=ml.p3dn.24xlarge-,ml.g4dn.xlarge,-%3E%3E%20Fast%20launch) because it is the only GPU with fast launch\n",
        "  1. Uses [Elastic Inference](https://aws.amazon.com/machine-learning/elastic-inference/)\n",
        "\n",
        "1. [EC2 Instances](https://aws.amazon.com/ec2/instance-types/)\n",
        "\n",
        "Note: If you try to add Elastic inference you might need to request a service\n",
        "limit, which may take a few days.\n",
        "\n",
        "https://stackoverflow.com/questions/71738894/unable-to-create-aws-segamaker-error-the-account-level-service-limit-number-o\n",
        "\n",
        "https://support.console.aws.amazon.com/support/home?region=us-east-1&skipRegion=true#/case/create?issueType=service-limit-increase\n",
        "\n",
        "https://discuss.huggingface.co/t/deploying-open-ais-whisper-on-sagemaker/24761/16\n",
        "\n",
        "https://stackoverflow.com/questions/56255154/how-to-use-a-pretrained-model-from-s3-to-predict-some-data"
      ],
      "metadata": {
        "id": "KwmJ6IOHRBvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sagemaker\n",
        "\n"
      ],
      "metadata": {
        "id": "_lwHh7i6YxGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "from sagemaker.tensorflow import TensorFlowModel\n",
        "from sagemaker.utils import S3DataConfig\n",
        "\n",
        "import shutil\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
        "\n",
        "role = sagemaker.get_execution_role()\n",
        "sm_session = sagemaker.Session()\n",
        "bucket_name = sm_session.default_bucket()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "M5xeV7nnaykI",
        "outputId": "ecbceedf-19b9-47a1-d41b-07e8d2d7ccde"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b00ebee3244a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_execution_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msm_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mbucket_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mget_execution_role\u001b[0;34m(sagemaker_session)\u001b[0m\n\u001b[1;32m   5039\u001b[0m     \"\"\"\n\u001b[1;32m   5040\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5041\u001b[0;31m         \u001b[0msagemaker_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5042\u001b[0m     \u001b[0marn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_caller_identity_arn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client, default_bucket, settings, sagemaker_metrics_client)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         self._initialize(\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mboto_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0msagemaker_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client, sagemaker_metrics_client)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_region_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregion_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_region_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"Must setup local AWS configuration with a region supported by SageMaker.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Must setup local AWS configuration with a region supported by SageMaker."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMf/SbkjPmOYyLutzaQvyXd",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}